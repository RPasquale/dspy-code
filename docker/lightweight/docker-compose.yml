services:
    dspy-agent:
      image: dspy-lightweight:latest
      user: "${DSPY_UID:-10001}:${DSPY_GID:-10001}"
      working_dir: /workspace
      build:
        context: ../..
        dockerfile: docker/lightweight/Dockerfile
      environment:
        - LOCAL_MODE=false
        - USE_OLLAMA=true
        - DB_BACKEND=reddb
        - DSPY_AUTO_APPLY_PATCHES=0
        - REDDB_URL=http://reddb:8080
        - REDDB_NAMESPACE=dspy
        - REDDB_TOKEN=${REDDB_ADMIN_TOKEN}
        - MODEL_NAME=${MODEL_NAME:-qwen3:1.7b}
        - OPENAI_API_KEY=
        - OPENAI_BASE_URL=http://ollama:11434
        - OLLAMA_MODEL=${OLLAMA_MODEL:-qwen3:1.7b}
        - OLLAMA_API_KEY=
        - OLLAMA_MODELS=${OLLAMA_MODELS:-qwen3:1.7b,deepseek-coder:1.3b}
        - OLLAMA_MAX_TOKENS=${OLLAMA_MAX_TOKENS:-0}
        - OLLAMA_KEEP_ALIVE
        - OLLAMA_ALLOW_MISSING_MODEL
        - OLLAMA_TAG_TIMEOUT
        - DSPY_LM_TIMEOUT
        - DSPY_OLLAMA_TIMEOUT
        - DSPY_OPENAI_TIMEOUT
        - DSPY_LM_MAX_RETRIES
        - DSPY_LM_NUM_RETRIES
        - KAFKA_BOOTSTRAP_SERVERS=kafka:9092
        - KAFKA_CLIENT_ID=dspy-agent
        - KAFKA_TOPIC_PREFIX=
        # Cache configuration to fix SQLite syntax errors
        - DSPY_CACHE_DIR=/workspace/.dspy_cache
        - DISKCACHE_DISABLE_SQLITE_OPTIMIZATIONS=1
        - DISKCACHE_DISABLE_SQLITE_PRAGMA=1
        - DISKCACHE_DISABLE_SQLITE_WAL=1
        - DISKCACHE_DISABLE_SQLITE_JOURNAL=1
        # Advanced features
        - DSPY_ENABLE_AUTO_SCALING=true
        - DSPY_PERFORMANCE_MODE=optimized
        - DSPY_INTELLIGENT_CACHING=true
        - DSPY_ADAPTIVE_LEARNING=true
        - PYTHONPATH=/workspace
        - MESH_ENDPOINT=${MESH_WORKER_ENDPOINT:-http://mesh-worker:50052}
        - MESH_NODE_ID=${MESH_NODE_ID:-9002}
        - MESH_DOMAIN=${MESH_GATEWAY_DOMAIN:-edge}
        - MESH_SERVICES_FILE=${MESH_SERVICES_FILE:-/etc/dspy/mesh-services.json}
        # Docker environment flag to override dspy.IS_STUB
        - DOCKER_ENV=true
        # Disable background auto-training loops for interactive sessions
        - DSPY_AUTO_TRAIN=0
      entrypoint: ["/entrypoints/run_dspy_agent.sh"]
      volumes:
        - ${WORKSPACE_DIR}:/workspace:rw
        - ${WORKSPACE_DIR}/logs:/workspace/logs:rw
        - ./entrypoints:/entrypoints:ro
        - dspy-cache:/home/agent/.dspy_cache:rw
        - ./mesh-services.json:/etc/dspy/mesh-services.json:ro
      ports:
        - "127.0.0.1:8765:8765"
      restart: unless-stopped
      security_opt:
        - no-new-privileges:true
      read_only: true
      tmpfs:
        - /tmp
      cap_drop:
        - ALL
      deploy:
        resources:
          limits:
            memory: 4G
            cpus: '2.0'
          reservations:
            memory: 2G
            cpus: '1.0'
        restart_policy:
          condition: on-failure
          delay: 5s
          max_attempts: 3
          window: 120s
      depends_on:
        kafka:
          condition: service_healthy
        reddb:
          condition: service_healthy
        mesh-hub:
          condition: service_healthy
        mesh-worker:
          condition: service_healthy

    ollama:
      image: ollama/ollama:latest
      entrypoint: ["/bin/sh", "/entrypoints/run_ollama.sh"]
      ports:
        - "127.0.0.1:11435:11434"
      volumes:
        - ollama:/root/.ollama
        - ./entrypoints:/entrypoints:ro
      environment:
        - OLLAMA_MAX_LOADED_MODELS=1
        - OLLAMA_NUM_PARALLEL=1
        - OLLAMA_MAX_QUEUE=512
        - OLLAMA_MODELS=${OLLAMA_MODELS:-qwen3:1.7b,deepseek-coder:1.3b}
        - OLLAMA_STARTUP_DELAY
      deploy:
        resources:
          limits:
            memory: 6G
            cpus: '4.0'
          reservations:
            memory: 3G
            cpus: '2.0'
        restart_policy:
          condition: on-failure
          delay: 10s
          max_attempts: 3
          window: 300s
      healthcheck:
        test: ["CMD-SHELL", "ollama list >/dev/null 2>&1 || exit 1"]
        interval: 10s
        timeout: 5s
        retries: 20
        start_period: 10s

    zookeeper:
      image: confluentinc/cp-zookeeper:7.5.0
      environment:
        - ZOOKEEPER_CLIENT_PORT=2181
        - ZOOKEEPER_TICK_TIME=2000
        - ZOOKEEPER_SYNC_LIMIT=2
        - ZOOKEEPER_4LW_COMMANDS_WHITELIST=srvr,ruok
      ports:
        - "127.0.0.1:2181:2181"
      healthcheck:
        test: ["CMD-SHELL", "echo srvr | nc localhost 2181 | grep Mode"]
        interval: 10s
        timeout: 5s
        retries: 20
        start_period: 15s
      deploy:
        resources:
          limits:
            memory: 1G
            cpus: '1.0'
          reservations:
            memory: 512M
            cpus: '0.5'

    kafka:
      image: confluentinc/cp-kafka:7.5.0
      depends_on:
        zookeeper:
          condition: service_healthy
      environment:
        - KAFKA_BROKER_ID=1
        - KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181
        - KAFKA_LISTENERS=PLAINTEXT://0.0.0.0:9092,PLAINTEXT_HOST://0.0.0.0:29092
        - KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092,PLAINTEXT_HOST://localhost:29092
        - KAFKA_LISTENER_SECURITY_PROTOCOL_MAP=PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
        - KAFKA_INTER_BROKER_LISTENER_NAME=PLAINTEXT
        - KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1
        - KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR=1
        - KAFKA_TRANSACTION_STATE_LOG_MIN_ISR=1
        - KAFKA_AUTO_CREATE_TOPICS_ENABLE=true
        - KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS=0
      ports:
        - "127.0.0.1:9092:29092"
      healthcheck:
        test: ["CMD-SHELL", "kafka-broker-api-versions --bootstrap-server localhost:9092 || exit 1"]
        interval: 15s
        timeout: 10s
        retries: 20
        start_period: 30s
      deploy:
        resources:
          limits:
            memory: 2G
            cpus: '2.0'
          reservations:
            memory: 1G
            cpus: '1.0'

    mesh-hub:
      build:
        context: ../..
        dockerfile: docker/lightweight/mesh-core/Dockerfile
      environment:
        - MESH_NODE_ID=${MESH_HUB_NODE_ID:-9001}
        - MESH_DOMAIN=${MESH_DOMAIN:-1}
        - MESH_DOMAIN_ID=${MESH_DOMAIN_ID:-1}
        - MESH_LISTEN_ADDR=0.0.0.0:7000
        - MESH_GRPC_LISTEN_ADDR=0.0.0.0:50051
        - MESH_METRICS_ADDR=0.0.0.0:9100
        - MESH_PEERS=
        - MESH_EXTRA_ARGS=${MESH_EXTRA_ARGS:-}
      ports:
        - "127.0.0.1:50051:50051"
        - "127.0.0.1:9100:9100"
      healthcheck:
        test: ["CMD-SHELL", "nc -z localhost 50051"]
        interval: 10s
        timeout: 5s
        retries: 10
        start_period: 10s
      deploy:
        resources:
          limits:
            memory: 1G
            cpus: '1.0'
          reservations:
            memory: 256M
            cpus: '0.25'

    mesh-worker:
      build:
        context: ../..
        dockerfile: docker/lightweight/mesh-core/Dockerfile
      environment:
        - MESH_NODE_ID=${MESH_NODE_ID:-9002}
        - MESH_DOMAIN=${MESH_DOMAIN:-1}
        - MESH_DOMAIN_ID=${MESH_DOMAIN_ID:-1}
        - MESH_LISTEN_ADDR=0.0.0.0:7001
        - MESH_GRPC_LISTEN_ADDR=0.0.0.0:50052
        - MESH_METRICS_ADDR=0.0.0.0:9101
        - MESH_PEERS=mesh-hub:7000
        - MESH_EXTRA_ARGS=${MESH_EXTRA_ARGS:-}
      ports:
        - "127.0.0.1:50052:50052"
        - "127.0.0.1:9101:9101"
      depends_on:
        mesh-hub:
          condition: service_healthy
      healthcheck:
        test: ["CMD-SHELL", "nc -z localhost 50052"]
        interval: 10s
        timeout: 5s
        retries: 10
        start_period: 10s
      deploy:
        resources:
          limits:
            memory: 1G
            cpus: '1.0'
          reservations:
            memory: 256M
            cpus: '0.25'

    mesh-trainer:
      build:
        context: ../..
        dockerfile: docker/lightweight/mesh-core/Dockerfile
      environment:
        - MESH_NODE_ID=${MESH_TRAINER_NODE_ID:-9003}
        - MESH_DOMAIN=${MESH_DOMAIN:-1}
        - MESH_DOMAIN_ID=${MESH_DOMAIN_ID:-1}
        - MESH_LISTEN_ADDR=0.0.0.0:7002
        - MESH_GRPC_LISTEN_ADDR=0.0.0.0:50053
        - MESH_METRICS_ADDR=0.0.0.0:9102
        - MESH_PEERS=mesh-hub:7000
        - MESH_EXTRA_ARGS=${MESH_EXTRA_ARGS:-}
      ports:
        - "127.0.0.1:50053:50053"
        - "127.0.0.1:9102:9102"
      depends_on:
        mesh-hub:
          condition: service_healthy
      healthcheck:
        test: ["CMD-SHELL", "nc -z localhost 50053"]
        interval: 10s
        timeout: 5s
        retries: 10
        start_period: 10s
      deploy:
        resources:
          limits:
            memory: 1G
            cpus: '1.0'
          reservations:
            memory: 256M
            cpus: '0.25'

    mesh-gateway:
      build:
        context: ../..
        dockerfile: docker/lightweight/mesh-core/Dockerfile
      environment:
        - MESH_NODE_ID=${MESH_GATEWAY_NODE_ID:-9010}
        - MESH_DOMAIN=${MESH_DOMAIN:-1}
        - MESH_DOMAIN_ID=${MESH_DOMAIN_ID:-1}
        - MESH_LISTEN_ADDR=0.0.0.0:7003
        - MESH_GRPC_LISTEN_ADDR=0.0.0.0:50060
        - MESH_METRICS_ADDR=0.0.0.0:9103
        - MESH_PEERS=mesh-hub:7000
        - MESH_EXTRA_ARGS=${MESH_EXTRA_ARGS:-}
      depends_on:
        mesh-hub:
          condition: service_healthy
      ports:
        - "127.0.0.1:50060:50060"
        - "127.0.0.1:9103:9103"
      healthcheck:
        test: ["CMD-SHELL", "nc -z localhost 50060"]
        interval: 10s
        timeout: 5s
        retries: 10
        start_period: 10s
      deploy:
        resources:
          limits:
            memory: 1G
            cpus: '1.0'
          reservations:
            memory: 256M
            cpus: '0.25'

    prometheus:
      image: prom/prometheus:latest
      volumes:
        - ./prometheus.yml:/etc/prometheus/prometheus.yml:ro
      ports:
        - "127.0.0.1:9090:9090"
      depends_on:
        mesh-hub:
          condition: service_healthy
        mesh-worker:
          condition: service_healthy
        mesh-trainer:
          condition: service_healthy
        mesh-gateway:
          condition: service_healthy

    redis:
      image: redis:7-alpine
      command: ["redis-server","--save","","--appendonly","no"]
      ports:
        - "127.0.0.1:6379:6379"
      volumes:
        - redis-data:/data
      healthcheck:
        test: ["CMD-SHELL", "redis-cli ping | grep PONG"]
        interval: 10s
        timeout: 5s
        retries: 5
        start_period: 5s
      restart: unless-stopped
      deploy:
        resources:
          limits:
            memory: 2G
            cpus: '1.0'
          reservations:
            memory: 512M
            cpus: '0.5'

    reddb:
      image: dspy-lightweight:latest
      container_name: reddb
      working_dir: /app
      entrypoint: ["/usr/local/bin/reddb"]
      restart: unless-stopped
      user: root
      ports:
        - "127.0.0.1:8082:8080"
      environment:
        - REDDB_ADMIN_TOKEN=${REDDB_ADMIN_TOKEN}
        - REDDB_DATA_DIR=/data
      volumes:
        - ./data/reddb:/data
      healthcheck:
        test: ["CMD-SHELL", "curl -f http://localhost:8080/health || exit 1"]
        interval: 10s
        timeout: 5s
        retries: 5
        start_period: 10s
      deploy:
        resources:
          limits:
            memory: 1G
            cpus: '1.0'
          reservations:
            memory: 512M
            cpus: '0.5'
      security_opt:
        - no-new-privileges:true
      read_only: true
      tmpfs:
        - /tmp
      cap_drop:
        - ALL

    fastapi-backend:
      image: dspy-lightweight:latest
      # user: "${DSPY_UID:-10001}:${DSPY_GID:-10001}"  # Temporarily disabled for permission fix
      working_dir: /app
      entrypoint: ["/bin/sh", "-lc", "python -m dspy_agent.server.fastapi_backend"]
      environment:
        - REDDB_URL=http://reddb:8080
        - REDDB_NAMESPACE=dspy
        - REDDB_TOKEN=${REDDB_ADMIN_TOKEN}
        - DSPY_WORKSPACE=/workspace
        - DSPY_LOGS=/workspace/logs
        - LOGS_DIR=/workspace/logs
      volumes:
        - ${WORKSPACE_DIR}:/workspace:rw
        - ./entrypoints:/entrypoints:ro
        - ./logs:/app/logs:rw
      ports:
        - "127.0.0.1:8767:8767"
      depends_on:
        reddb:
          condition: service_healthy
      healthcheck:
        test: ["CMD-SHELL", "python - <<'PY'\nimport urllib.request as u, sys\ntry:\n u.urlopen('http://localhost:8767/api/db/health')\n sys.exit(0)\nexcept Exception:\n sys.exit(1)\nPY"]
        interval: 20s
        timeout: 5s
        retries: 10
        start_period: 5s
      restart: unless-stopped
      security_opt:
        - no-new-privileges:true
      read_only: false
      tmpfs:
        - /tmp
      cap_drop:
        - ALL

    dspy-embedder:
      image: dspy-lightweight:latest
      entrypoint: ["uvicorn", "dspy_agent.embedding.dspy_embedder_service:app", "--host", "0.0.0.0", "--port", "8080"]
      environment:
        - DSPY_EMBED_MODEL=${DSPY_EMBED_MODEL:-sentence-transformers/all-MiniLM-L6-v2}
        - DSPY_EMBED_NORMALIZE=${DSPY_EMBED_NORMALIZE:-0}
        - DSPY_GEPA_DATASET=${DSPY_GEPA_DATASET:-}
        - DSPY_GEPA_AUTO=${DSPY_GEPA_AUTO:-light}
        - DSPY_GEPA_LOG_DIR=${DSPY_GEPA_LOG_DIR:-}
        - DSPY_LM_MODEL=${DSPY_LM_MODEL:-}
        - DSPY_LM_PROVIDER=${DSPY_LM_PROVIDER:-openai}
        - OPENAI_API_KEY=${OPENAI_API_KEY:-}
        - OPENAI_BASE_URL=${OPENAI_BASE_URL:-}
      volumes:
        - ${WORKSPACE_DIR}:/workspace
      ports:
        - "127.0.0.1:18082:8080"
      restart: unless-stopped
      healthcheck:
        test: ["CMD-SHELL", "curl -sf http://localhost:8080/health || exit 1"]
        interval: 15s
        timeout: 5s
        retries: 10
        start_period: 10s

    infermesh-node-a:
      image: official-infermesh:latest
      entrypoint: ["/usr/local/bin/meshd", "start", "--config", "/config/node-a.yaml"]
      volumes:
        - ./infermesh-config/node-a.yaml:/config/node-a.yaml:ro
        - infermesh-node-a-data:/app/data
      restart: unless-stopped
      healthcheck:
        test: ["CMD-SHELL", "curl -sf http://localhost:8080/health || exit 1"]
        interval: 15s
        timeout: 5s
        retries: 10
        start_period: 10s

    infermesh-node-b:
      image: official-infermesh:latest
      entrypoint: ["/usr/local/bin/meshd", "start", "--config", "/config/node-b.yaml"]
      volumes:
        - ./infermesh-config/node-b.yaml:/config/node-b.yaml:ro
        - infermesh-node-b-data:/app/data
      restart: unless-stopped
      healthcheck:
        test: ["CMD-SHELL", "curl -sf http://localhost:8080/health || exit 1"]
        interval: 15s
        timeout: 5s
        retries: 10
        start_period: 10s

    infermesh-router:
      image: official-infermesh:latest
      entrypoint: ["/entrypoints/run_official_router.sh"]
      environment:
        - AGENT_SERVICE=infermesh-node-a:50051
        - ROUTER_HTTP_PORT=9000
        - ROUTER_GRPC_PORT=9100
        - ROUTER_MAX_REQUESTS=512
        - ROUTER_REQUEST_TIMEOUT=30
        - DSPY_EMBEDDER_URL=http://dspy-embedder:8080/embed
      volumes:
        - ./entrypoints:/entrypoints:ro
      depends_on:
        infermesh-node-a:
          condition: service_healthy
        dspy-embedder:
          condition: service_healthy
      ports:
        - "127.0.0.1:${INFERMESH_HOST_PORT:-19000}:9000"
      healthcheck:
        test: ["CMD-SHELL", "curl -sf http://localhost:9000/health || exit 1"]
        interval: 15s
        timeout: 5s
        retries: 10
        start_period: 10s
      restart: unless-stopped

    # Dedicated dashboard service (serves agent dashboard on port 8081)
    dashboard:
      image: dspy-lightweight:latest
      user: "${DSPY_UID:-10001}:${DSPY_GID:-10001}"
      working_dir: /app
      entrypoint: ["bash", "-lc", "python3 -m scripts.agent_dashboard_panel --interval 2"]
      environment:
        - WORKSPACE_DIR=/workspace
        - DSPY_WORKSPACE=/workspace
        - PYTHONPATH=/workspace
      volumes:
        - ${WORKSPACE_DIR}:/workspace:rw
        - ./entrypoints:/entrypoints:ro
      ports:
        - "127.0.0.1:18081:8081"
      restart: unless-stopped
      security_opt:
        - no-new-privileges:true
      # Allow server to write .dspy_reports under /app
      read_only: false
      cap_drop:
        - ALL
      depends_on:
        kafka:
          condition: service_healthy

    embed-worker:
      build:
        context: ../..
        dockerfile: docker/lightweight/embed_worker.Dockerfile
      depends_on:
        kafka:
          condition: service_healthy
        infermesh-router:
          condition: service_started
      environment:
        - KAFKA_BOOTSTRAP_SERVERS=kafka:9092
        - EMBED_INPUT_TOPIC=embedding_input
        - EMBED_OUTPUT_TOPIC=embeddings
        - EMBED_GROUP=embed-worker
        - EMBED_BACKEND=infermesh
        - INFERMESH_URL=http://infermesh-router:9000
        - EMBED_MODEL=${INFERMESH_MODEL:-BAAI/bge-small-en-v1.5}
        - EMBED_BATCH_SIZE=${EMBED_BATCH_SIZE:-64}
        - EMBED_MAX_WAIT_SEC=${EMBED_MAX_WAIT_SEC:-0.5}
        - EMBED_WRITE_PARQUET=1
        - EMBED_NORMALIZE=${EMBED_NORMALIZE:-0}
        - EMBED_PARQUET_DIR=/workspace/vectorized/embeddings
        - INFERMESH_ROUTING_STRATEGY=${INFERMESH_ROUTING_STRATEGY:-hybrid}
        - INFERMESH_CACHE_TTL=${INFERMESH_CACHE_TTL:-300}
        - REDDB_URL
        - REDDB_NAMESPACE=dspy
        - REDDB_TOKEN
        - REDDB_STREAM=embeddings
        - REDDB_MODE=stream
        - EMBED_METRICS_PORT=9100
      volumes:
        - ${WORKSPACE_DIR}:/workspace
        - ./dspy_agent/streaming:/app/dspy_agent/streaming:ro
        - ./entrypoints:/entrypoints:ro
        - hf-cache:/root/.cache/huggingface
      entrypoint: ["/bin/sh", "-lc", "sh /entrypoints/run_embed_worker.sh"]
      ports:
        - "127.0.0.1:19101:9100"
      restart: unless-stopped
      security_opt:
        - no-new-privileges:true
      read_only: true
      tmpfs:
        - /tmp
      cap_drop:
        - ALL
      healthcheck:
        test: ["CMD-SHELL", "python -c \"import urllib.request as u; u.urlopen('http://localhost:9100/health')\" >/dev/null 2>&1 || exit 1"]
        interval: 20s
        timeout: 5s
        retries: 10
        start_period: 15s

    spark:
      image: apache/spark:3.5.0
      depends_on:
        kafka:
          condition: service_healthy
      user: "0:0"
      environment:
        - HOME=/tmp
        - USER=spark
        - SPARK_USER=spark
        - HADOOP_USER_NAME=spark
        - SPARK_LOCAL_DIRS=/tmp
        - SPARK_CONF_DIR=/opt/bitnami/spark/conf
      volumes:
        - ${WORKSPACE_DIR}:/workspace
        - ./dspy_agent/streaming:/app/dspy_agent/streaming:ro
        - ./entrypoints:/entrypoints:ro
      entrypoint: ["/entrypoints/run_spark.sh"]
      healthcheck:
        test: ["CMD-SHELL", "pgrep -f 'spark_logs.py' >/dev/null 2>&1 || exit 1"]
        interval: 30s
        timeout: 5s
        retries: 10
        start_period: 20s

    dspy-worker:
      image: dspy-lightweight:latest
      user: "${DSPY_UID:-10001}:${DSPY_GID:-10001}"
      depends_on:
        kafka:
          condition: service_healthy
      environment:
        # Cache configuration to fix SQLite syntax errors
        - DSPY_CACHE_DIR=/workspace/.dspy_cache
        - DISKCACHE_DISABLE_SQLITE_OPTIMIZATIONS=1
        - DISKCACHE_DISABLE_SQLITE_PRAGMA=1
        - DISKCACHE_DISABLE_SQLITE_WAL=1
        - DISKCACHE_DISABLE_SQLITE_JOURNAL=1
      entrypoint: ["/entrypoints/run_worker.sh", "app"]
      volumes:
        - ${WORKSPACE_DIR}:/workspace:rw
        - ./entrypoints:/entrypoints:ro
        - dspy-cache:/home/agent/.dspy_cache:rw
      restart: unless-stopped
      security_opt:
        - no-new-privileges:true
      read_only: true
      tmpfs:
        - /tmp
      cap_drop:
        - ALL
      healthcheck:
        test: ["CMD-SHELL", "pgrep -f 'dspy-agent worker --topic app' >/dev/null 2>&1 || exit 1"]
        interval: 15s
        timeout: 5s
        retries: 10
        start_period: 10s

    dspy-worker-backend:
      image: dspy-lightweight:latest
      user: "${DSPY_UID:-10001}:${DSPY_GID:-10001}"
      depends_on:
        kafka:
          condition: service_healthy
      entrypoint: ["/entrypoints/run_worker.sh", "backend"]
      volumes:
        - ${WORKSPACE_DIR}:/workspace:rw
        - ./entrypoints:/entrypoints:ro
        - dspy-cache:/home/agent/.dspy_cache:rw
      restart: unless-stopped
      security_opt:
        - no-new-privileges:true
      read_only: true
      tmpfs:
        - /tmp
      cap_drop:
        - ALL
      healthcheck:
        test: ["CMD-SHELL", "pgrep -f 'dspy-agent worker --topic backend' >/dev/null 2>&1 || exit 1"]
        interval: 15s
        timeout: 5s
        retries: 10
        start_period: 10s

    dspy-worker-frontend:
      image: dspy-lightweight:latest
      user: "${DSPY_UID:-10001}:${DSPY_GID:-10001}"
      depends_on:
        kafka:
          condition: service_healthy
      environment:
        # Cache configuration to fix SQLite syntax errors
        - DSPY_CACHE_DIR=/workspace/.dspy_cache
        - DISKCACHE_DISABLE_SQLITE_OPTIMIZATIONS=1
        - DISKCACHE_DISABLE_SQLITE_PRAGMA=1
        - DISKCACHE_DISABLE_SQLITE_WAL=1
        - DISKCACHE_DISABLE_SQLITE_JOURNAL=1
      entrypoint: ["/entrypoints/run_worker.sh", "frontend"]
      volumes:
        - ${WORKSPACE_DIR}:/workspace:rw
        - ./entrypoints:/entrypoints:ro
        - dspy-cache:/home/agent/.dspy_cache:rw
      restart: unless-stopped
      security_opt:
        - no-new-privileges:true
      read_only: true
      tmpfs:
        - /tmp
      cap_drop:
        - ALL
      healthcheck:
        test: ["CMD-SHELL", "pgrep -f 'dspy-agent worker --topic frontend' >/dev/null 2>&1 || exit 1"]
        interval: 15s
        timeout: 5s
        retries: 10
        start_period: 10s

    # One-off test runner for the lightweight image. Mounts your workspace and
    # installs pytest into HOME (/workspace) to keep the image minimal.
    agent-tests:
      image: dspy-lightweight:latest
      environment:
        - HOME=/workspace
      working_dir: /workspace
      volumes:
        - ${WORKSPACE_DIR}:/workspace:rw
      entrypoint: ["/bin/sh", "-lc"]
      command: |
        "python -m pip install --user -q pytest && \
         python -m pytest -q"
      restart: "no"
      security_opt:
        - no-new-privileges:true
      read_only: true
      tmpfs:
        - /tmp
      cap_drop:
        - ALL
      profiles:
        - testing

    dspy-code-watch:
      image: dspy-lightweight:latest
      user: "${DSPY_UID:-10001}:${DSPY_GID:-10001}"
      depends_on:
        kafka:
          condition: service_healthy
      environment:
        - PYTHONPATH=/workspace
      entrypoint: ["/entrypoints/run_code_watch.sh"]
      volumes:
        - ${WORKSPACE_DIR}:/workspace:rw
        - ./entrypoints:/entrypoints:ro
      restart: unless-stopped
      security_opt:
        - no-new-privileges:true
      read_only: true
      tmpfs:
        - /tmp
      cap_drop:
        - ALL
      healthcheck:
        test: ["CMD-SHELL", "pgrep -f 'python' >/dev/null 2>&1 || exit 1"]
        interval: 15s
        timeout: 5s
        retries: 10
        start_period: 10s

    dspy-code-indexer:
      image: dspy-lightweight:latest
      user: "${DSPY_UID:-10001}:${DSPY_GID:-10001}"
      depends_on:
        kafka:
          condition: service_healthy
      environment:
        - PYTHONPATH=/workspace
      entrypoint: ["/entrypoints/run_code_indexer.sh"]
      volumes:
        - ${WORKSPACE_DIR}:/workspace:rw
        - ./entrypoints:/entrypoints:ro
      restart: unless-stopped
      security_opt:
        - no-new-privileges:true
      read_only: true
      tmpfs:
        - /tmp
      cap_drop:
        - ALL
      healthcheck:
        test: ["CMD-SHELL", "pgrep -f 'python' >/dev/null 2>&1 || exit 1"]
        interval: 15s
        timeout: 5s
        retries: 10
        start_period: 10s

    dspy-router:
      image: dspy-lightweight:latest
      user: "${DSPY_UID:-10001}:${DSPY_GID:-10001}"
      depends_on:
        kafka:
          condition: service_healthy
      entrypoint: ["/entrypoints/run_router.sh"]
      volumes:
        - ./entrypoints:/entrypoints:ro
      healthcheck:
        test: ["CMD-SHELL", "pgrep -f 'router_worker' >/dev/null 2>&1 || exit 1"]
        interval: 15s
        timeout: 5s
        retries: 10
        start_period: 10s

    rl-trainer:
      image: dspy-lightweight:latest
      user: "${DSPY_UID:-10001}:${DSPY_GID:-10001}"
      entrypoint: ["/bin/bash","-lc"]
      command: >-
        python -m dspy_agent.training.entrypoint \
          --workspace /workspace \
          --signature CodeContextSig \
          --verifiers dspy_agent.verifiers.custom \
          --steps 200 \
          --env development
      volumes:
        - ${WORKSPACE_DIR}:/workspace:rw
      restart: on-failure
      security_opt:
        - no-new-privileges:true
      read_only: true
      tmpfs:
        - /tmp
      cap_drop:
        - ALL
      profiles:
        - testing

    emb-indexer:
      build:
        context: ../..
        dockerfile: docker/lightweight/embed_worker.Dockerfile
      depends_on:
        kafka:
          condition: service_healthy
      environment:
        - KAFKA_BOOTSTRAP_SERVERS=kafka:9092
        - EMB_INDEX_TOPIC=embeddings
        - EMB_INDEX_GROUP=emb-indexer
        - REDDB_URL
        - REDDB_NAMESPACE=dspy
        - REDDB_TOKEN
        - REDDB_INDEX_STREAM=emb_index
      volumes:
        - ${WORKSPACE_DIR}:/workspace
        - ./dspy_agent/streaming:/app/dspy_agent/streaming:ro
        - ./entrypoints:/entrypoints:ro
      entrypoint: ["/bin/sh", "-lc", "sh /entrypoints/run_emb_indexer.sh"]
      restart: unless-stopped
      security_opt:
        - no-new-privileges:true
      read_only: true
      tmpfs:
        - /tmp
      cap_drop:
        - ALL
    spark-vectorizer:
      build:
        context: .
        dockerfile: spark_vectorizer.Dockerfile
      depends_on:
        kafka:
          condition: service_healthy
      user: "0:0"
      environment:
        - HOME=/tmp
        - USER=spark
        - SPARK_USER=spark
        - HADOOP_USER_NAME=spark
        - SPARK_LOCAL_DIRS=/tmp
        - SPARK_CONF_DIR=/opt/bitnami/spark/conf
        - KAFKA_BOOTSTRAP=kafka:9092
        - SPARK_KAFKA_TOPICS=code.fs.events
        - VEC_OUTPUT_DIR=/workspace/vectorized/embeddings
        - VEC_CHECKPOINT=/workspace/.dspy_checkpoints/vectorizer
        - SINK_INPUT_TOPIC=embedding_input
        # Additional startup delay to ensure Kafka is fully ready
        - SPARK_STARTUP_DELAY=10
      volumes:
        - ${WORKSPACE_DIR}:/workspace
        - ./dspy_agent/streaming:/app/dspy_agent/streaming:ro
        - ./entrypoints:/entrypoints:ro
      entrypoint: ["/bin/bash", "-lc", "source /entrypoints/run_spark_vectorizer.sh"]
      ports:
        - "127.0.0.1:4041:4040"
      restart: unless-stopped
      security_opt:
        - no-new-privileges:true
      read_only: false
      cap_drop:
        - ALL
      healthcheck:
        test: ["CMD-SHELL", "exit 0"]
        interval: 30s
        timeout: 5s
        retries: 1
        start_period: 1s

    smoke:
      build:
        context: ../..
        dockerfile: docker/lightweight/embed_worker.Dockerfile
      depends_on:
        kafka:
          condition: service_healthy
        # spark-vectorizer is optional for local dev; don't block on health
        spark-vectorizer:
          condition: service_started
        embed-worker:
          condition: service_healthy
      environment:
        - KAFKA_BOOTSTRAP=kafka:9092
        - RESULTS_TOPIC=agent.results
        - EMBED_PARQUET_DIR=/workspace/vectorized/embeddings_imesh
        - N_MESSAGES=5
        - SLEEP_SEC=6
      volumes:
        - ${WORKSPACE_DIR}:/workspace
        - ./entrypoints:/entrypoints:ro
      entrypoint: ["/bin/sh", "-lc", "sh /entrypoints/run_smoke.sh"]
      restart: "no"
      security_opt:
        - no-new-privileges:true
      read_only: true
      tmpfs:
        - /tmp
      cap_drop:
        - ALL
      profiles:
        - testing

    test-suite:
      build:
        context: .
        dockerfile: test_suite.Dockerfile
      environment:
        - WORKSPACE_DIR=${WORKSPACE_DIR}
      volumes:
        - ${WORKSPACE_DIR}:/workspace
        - ./logs:/workspace/logs
      depends_on:
        - dspy-agent
        - dashboard
      profiles:
        - testing

    # Intelligent auto-scaling service
    auto-scaler:
      image: dspy-lightweight:latest
      user: "${DSPY_UID:-10001}:${DSPY_GID:-10001}"
      working_dir: /app
      entrypoint: ["/entrypoints/run_auto_scaler.sh"]
      environment:
        - WORKSPACE_DIR=/workspace
        - KAFKA_BOOTSTRAP_SERVERS=kafka:9092
        - REDDB_URL=http://reddb:8080
        - REDDB_TOKEN=${REDDB_ADMIN_TOKEN}
        - AUTO_SCALER_INTERVAL=30
        - AUTO_SCALER_CPU_THRESHOLD=80
        - AUTO_SCALER_MEMORY_THRESHOLD=85
        - PYTHONPATH=/workspace
      volumes:
        - ${WORKSPACE_DIR}:/workspace:rw
        - ./entrypoints:/entrypoints:ro
      depends_on:
        kafka:
          condition: service_healthy
        reddb:
          condition: service_healthy
      restart: unless-stopped
      security_opt:
        - no-new-privileges:true
      read_only: true
      tmpfs:
        - /tmp
      cap_drop:
        - ALL
      healthcheck:
        test: ["CMD-SHELL", "pgrep -f 'python' >/dev/null 2>&1 || exit 1"]
        interval: 30s
        timeout: 5s
        retries: 5
        start_period: 10s

    # Go Orchestrator Service
    go-orchestrator:
      image: dspy-lightweight:latest
      build:
        context: ../..
        dockerfile: docker/lightweight/Dockerfile
      environment:
        - ORCHESTRATOR_PORT=9097
        - INFERMESH_BASE_URL=http://infermesh-router:9000
        - INFERMESH_API_KEY=${INFERMESH_API_KEY:-}
        - INFERMESH_MODEL=${INFERMESH_MODEL:-BAAI/bge-small-en-v1.5}
        - REDIS_URL=redis://redis:6379
        - METRICS_ENABLED=true
        - ORCHESTRATOR_DEMO=0
        - ENV_QUEUE_DIR=/app/logs/env_queue
      entrypoint: ["/usr/local/bin/orchestrator"]
      ports:
        - "127.0.0.1:9097:9097"
      volumes:
        - ./logs:/app/logs
      depends_on:
        - redis
        - infermesh-router
      restart: unless-stopped
      healthcheck:
        test: ["CMD-SHELL", "curl -f http://localhost:9097/metrics || exit 1"]
        interval: 10s
        timeout: 5s
        retries: 5
        start_period: 10s
      deploy:
        resources:
          limits:
            memory: 2G
            cpus: '2.0'
          reservations:
            memory: 512M
            cpus: '0.5'

    # Stream Supervisor Service
    stream-supervisor:
      image: dspy-lightweight:latest
      build:
        context: ../..
        dockerfile: docker/lightweight/Dockerfile
      environment:
        - SUPERVISOR_LISTEN=:7000
        - KAFKA_BROKERS=kafka:9092
        - INPUT_TOPIC=raw.events.demo
        - SUPERVISOR_GROUP=stream-supervisor
        - SUPERVISOR_METRICS_LISTEN=:9098
      entrypoint: ["/usr/local/bin/stream_supervisor"]
      ports:
        - "127.0.0.1:7000:7000"
        - "127.0.0.1:9098:9098"
      volumes:
        - ./logs:/app/logs
      depends_on:
        - kafka
        - redis
      restart: unless-stopped
      healthcheck:
        test: ["CMD-SHELL", "curl -f http://localhost:9098/metrics || exit 1"]
        interval: 10s
        timeout: 5s
        retries: 5
        start_period: 10s
      deploy:
        resources:
          limits:
            memory: 2G
            cpus: '2.0'
          reservations:
            memory: 512M
            cpus: '0.5'

    # Rust Environment Runner Service
    rust-env-runner:
      image: dspy-lightweight:latest
      build:
        context: ../..
        dockerfile: docker/lightweight/Dockerfile
      environment:
        - INFERMESH_BASE_URL=http://infermesh-router:9000
        - INFERMESH_API_KEY=${INFERMESH_API_KEY:-}
        - INFERMESH_MODEL=${INFERMESH_MODEL:-BAAI/bge-small-en-v1.5}
        - MAX_CONCURRENT_REQUESTS=100
        - BATCH_SIZE=512
        - CONNECTION_POOL_SIZE=50
        - REDIS_URL=redis://redis:6379
        - ENV_QUEUE_DIR=/app/logs/env_queue
        - METRICS_PORT=8083
        # - MESH_ENDPOINT=${MESH_WORKER_ENDPOINT:-http://mesh-worker:50052}
        # - MESH_NODE_ID=${MESH_NODE_ID:-9002}
        # - MESH_DOMAIN=${MESH_DOMAIN:-1}
        # - MESH_DOMAIN_ID=${MESH_DOMAIN_ID:-1}
        # - MESH_SERVICES_FILE=${MESH_SERVICES_FILE:-/etc/dspy/mesh-services.json}
        - KAFKA_BROKERS=kafka:9092
        - SUPERVISOR_GRPC_ADDR=http://stream-supervisor:7000
      entrypoint: ["/usr/local/bin/env_runner"]
      ports:
        - "127.0.0.1:8081:8083"
      volumes:
        - ./logs:/app/logs
        - ./mesh-services.json:/etc/dspy/mesh-services.json:ro
      depends_on:
        - redis
        - infermesh-router
        - go-orchestrator
        - stream-supervisor
      restart: unless-stopped
      healthcheck:
        test: ["CMD-SHELL", "curl -f http://localhost:8083/health || exit 1"]
        interval: 10s
        timeout: 5s
        retries: 5
        start_period: 15s
      deploy:
        resources:
          limits:
            memory: 4G
            cpus: '4.0'
          reservations:
            memory: 1G
            cpus: '1.0'


volumes:
  ollama: {}
  dspy-cache: {}
  hf-cache: {}
  reddb_data: {}
  redis-data: {}
  infermesh-node-a-data: {}
  infermesh-node-b-data: {}
